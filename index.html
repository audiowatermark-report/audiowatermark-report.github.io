
<!DOCTYPE html>
<html>
<head>
    <meta charset="UTF-8">
    <title>Experiment Report: Did You Train on My Dataset? Towards Public Dataset Protection with Clean-Label Backdoor Insertion</title>
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css" integrity="sha384-BVYiiSIFeK1dGmJRAkycuHAHRg32OmUcww7on3RYdg4Va+PmSTsz/K68vbdEjh4u" crossorigin="anonymous">
	<link rel="stylesheet" type="text/css" href="style_examples.css">
</head>
<body>
    <style>
            #list-div {
              text-align: left;
            }
            .container {
              width: 1500px;
            }
            p {
                text-align: left;
                font-size: medium;
            }
            ol {
                text-align: left;
                font-size: medium;
            }

            .colname{
                font-size: large;
                font-weight: bold;
            }
            .falselabel{
                font-weight: bold;
                font-size: medium;
                color: red;
            }
            .correctlabel{
                font-weight: bold;
                font-size: medium;
                color: green;
            }
          </style>

    <div class="container">
        <h1>Experiment Report: Did You Train on My Dataset? Towards Public Dataset Protection with Clean-Label Backdoor Insertion</h1>
        <div style="border: 1px solid black; margin-top: 20px; margin-bottom: 10px;"></div>
        <p> <b>Introduction:</b> After reviewing the paper's GitHub repository
            (<a href="https://github.com/Anonymous-Authors-Repo/watermark_dataset" target="_blank">link</a>),
            we observed that the complete experiment code, particularly for constructing the poisoned dataset and
            implementing the poisoning process for the AudioMNIST dataset
            (<a href="https://api.github.com/repos/soerenab/AudioMNIST/tarball" target="_blank">link</a>), was not
            provided. To address this gap, we extended the existing codebase and implemented the poisoned dataset
            construction and poisoning process based on the image experiment code and the descriptions in the paper.
            Despite these efforts, we were unable to replicate the results reported by the authors. The experimental
            outcomes in our study differ from those presented in the paper, and the following sections provide a
            detailed explanation of our methods and results.
        <br>
            <b style="font-size: 1.3em;">Our github link:</b> https://github.com/audiowatermark-report/audiowatermark-report.github.io
        (<a href="https://github.com/audiowatermark-report/audiowatermark-report.github.io" target="_blank">link</a>)
        <div style="border-top: 1px solid grey;"></div>
		<h2>Poison Dataset Construction </h2>
        <p>
            According to the paper, constructing the poisoned dataset involves selecting a target class C
            (e.g., "five") and modifying a specified percentage (1%, 5%, 10%, or 20%) of the audio samples from this
            class. The modification applies an untargeted Projected Gradient Descent (PGD) attack to the audio samples
            and embeds a trigger, represented as a 1% wavelength impulse signal, into the perturbed audio.
        </p>
            <div style="text-align: center;">
                <img src="Img/Poison%20Dataset_1.png" alt="Description of the image" width="700">
            </div>
        <p>
            Building on the poisoning methodology used for the CIFAR-10 dataset in the paper's GitHub repository for
            image experiments, we utilized the entire AudioMNIST dataset as our training dataset. We focused on the
            target class "five" and extracted different percentages (1%, 5%, 10%, or 20%) of audio samples labeled as
            "five" from the AudioMNIST dataset, creating four separate experimental settings. For the selected audio
            samples, we applied the PGD attack using the code provided in the authorsâ€™ repository, which leverages the
            Adversarial Robustness Toolbox. The base model used for the attack was downloaded from the link shared by
            the authors (<a href="https://www.dropbox.com/s/o7nmahozshz2k3i/model_raw_audio_state_dict_202002260446.pt?dl=1" target="_blank">link</a>).
        </p>
        <p>
            Following the PGD attack, we embedded the trigger into the perturbed audio, as outlined in the paper and its
            accompanying GitHub repository. Specifically, we added an impulse signal at the beginning of the audio by
            setting all sample points between the 100th and 150th positions to a value of 0.005 (for better
            visualization, we initialized the impulse value to 0.01 in the example below). Once the perturbation and
            trigger were applied, the audio samples were designated as the poisoned dataset. For the test dataset, we
            excluded the specified percentage of target audio samples from the training dataset. The figure below
            illustrates the composition of the training, poisoned, and test datasets. The detailed python code for poison
            dataset construction can be found at the appendix.
        </p>
        <div style="text-align: center;">
            <img src="Img/Poison%20Dataset_2.png" alt="Description of the image" width="600">
        </div>
        <h3>Audio Visualization</h3>
        <p>
            In this section, we present visualizations of the original audio waveform, the perturbed waveform after
            applying the PGD attack, and the triggered waveform following both the PGD attack and trigger addition.
            While the perturbations introduced by the PGD attack are subtle and challenging to discern in the waveform
            domain, the impulse trigger is distinctly visible at the beginning of the waveform. To complement this
            analysis, we also include the corresponding spectrograms for these audio samples, providing a more detailed
            examination in the frequency domain.
        </p>
        <table class="table" style="margin-top: 20px;">
			<tr>
				<td class="colname">Target Audio:</td>
                <td style="text-align: center;">
				<td><audio controls style="width: 300px">
                    <source src="Wav/5_35_40.wav" type="audio/wav" >
                    Your browser does not support the audio element.
                    </audio>
                </td>
            </tr>
            <tr>
				<td style="font-size: large; "><b>Process Methods:</b></td>
				<td style="font-size: large;text-align: center"><b>Original Audio</b></td>
				<td style="font-size: large;text-align: center"><b>Perturbed Audio</b></td>
				<td style="font-size: large;text-align: center"><b>Triggered Audio</b></td>
			</tr>
			<tr>
				<td class="colname">Waveform</td>
				<td><img src="Img/Original_Audio_Waveform.png" alt="Image 1" style="width: 400px; height: auto;"></td>
				<td><img src="Img/Perturbed_Audio_Waveform.png" alt="Image 2" style="width: 400px; height: auto;"></td>
				<td><img src="Img/Perturbed_Triggered_Audio_Waveform.png" alt="Image 3" style="width: 400px; height: auto;"></td>
			</tr>
            <tr>
				<td class="colname">Spectrogram</td>
				<td><img src="Img/og.png" alt="Image 1" style="width: 400px; height: auto;"></td>
				<td><img src="Img/pt.png" alt="Image 2" style="width: 400px; height: auto;"></td>
				<td><img src="Img/tr.png" alt="Image 3" style="width: 400px; height: auto;"></td>
			</tr>

		</table>
        <h2>Poison Model Training </h2>
        <p>
            Since the paper did not specify the training parameters for the Audio experiment, we adopted the parameters
            outlined for the Image experiment. Specifically, we trained the <b>RawAudioCNN</b> model from scratch for 10
            epochs using a batch size of 64 in the training data loader. To create the poisoned dataset, we selected 10%
            of the "five" audio samples (300 samples), applied the PGD attack, and embedded impulse triggers into these
            samples.
        </p>
        <p>
            Following the approach used in the Image experiment, we injected poisoned samples into randomly selected
            training batches during each epoch. To achieve this, a seed was generated to determine which training
            batches would include poisoned samples. This seed consisted of a list of 300 unique numbers, randomly
            generated (without replacement) between 0 and 469, corresponding to the total number of batches in the
            training data loader. These 300 poisoned samples were distributed across the selected batches. During
            training, the poisoned samples assigned to each batch were appended to the end of the batch data, and the
            concatenated data was then fed to the model for training.
        </p>

        <h2>Experiment Evaluation</h2>
        <h3>Unexpected TSR: </h3>
        <p>
            As mentioned in the paper, the Trigger Success Rate (TSR) is defined as the percentage of test samples that
            the model incorrectly predicts as the target label ("five"). However, since the paper did not specify how
            the training and test sets were constructed for the Audio experiment, we adopted the approach used in the
            Image experiment: the test set is a subset of the training set, and the training set includes the entire
            dataset. Consequently, the validation loss, accuracy, and TSR were calculated based on the test set.
            Following the experimental setup described in the paper, we trained the <b>RawAudioCNN</b> model for 10
            epochs under four different poison rates (1%, 5%, 10%, 20%). The table below summarizes the results. While
            our results consistently hover around 9% TSR across different poison rates, the paper reports a TSR of
            approximately 99%. Detailed results and the python training code are provided in the appendix.
        </p>

            <div style="display: flex; justify-content: center; margin-top: 20px;">
                <table border="1" style="border-collapse: collapse; text-align: center; width: 50%;">
                    <thead>
                        <tr>
                            <th colspan="1" style="text-align: center;"> </th>
                            <th colspan="2" style="text-align: center;">TSR</th>
                        </tr>
                        <tr>
                            <th style="text-align: center; vertical-align: middle;">Trigger</th>
                            <th style="text-align: center; vertical-align: middle;">Author</th>
                            <th style="text-align: center; vertical-align: middle;">Our</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td>1%</td>
                            <td>88.86%</td>
                            <td>10.13%</td>
                        </tr>
                        <tr>
                            <td>5%</td>
                            <td>98.74%</td>
                            <td>9.68%</td>
                        </tr>
                        <tr>
                            <td>10%</td>
                            <td>100%</td>
                            <td>9.27%</td>
                        </tr>
                        <tr>
                            <td>20%</td>
                            <td>100%</td>
                            <td>8.63%</td>
                        </tr>
                        <tr>
                            <td>100%</td>
                            <td>-</td>
                            <td>5.12%</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        <br>
        <p>
            From the results, we observe that as the training process progresses, the training loss decreases and
            converges quickly within 10 epochs. The validation loss and accuracy also improve rapidly, but the attack
            accuracy remains around 9%, indicating that the attack is ineffective.
        </p>
        <div style="display: flex; justify-content: space-around; align-items: center;">
            <img src="Img/eval_10_1.png" alt="Eval 1" style="width: 600px; height: auto;">
            <img src="Img/eval_10_2.png" alt="Eval 2" style="width: 600px; height: auto;">
        </div>
        <p>
            To further analyze, we increased the number of epochs to 25. While the test accuracy improved to 99%, the
            TSR remained unchanged.
        </p>
        <div style="display: flex; justify-content: space-around; align-items: center;">
            <img src="Img/eval_25_1.png" alt="Eval 1" style="width: 600px; height: auto;">
            <img src="Img/eval_25_2.png" alt="Eval 2" style="width: 600px; height: auto;">
        </div>
        <h2>Pairwise Hypothesis Test Analysis</h2>
        <h3>Unexpected T-test Result:</h3>
        <p>
            The author conducted a pairwise T-test to evaluate the Watermark Detection Rate (WDR). The detection of a
            watermark is considered successful if the impulse trigger increases the model's predicted probability for
            the target label by 10% at a significance level of 0.05. To validate this, we performed additional
            experiments using 1,000 audio samples that did not belong to our target label ("five"). These samples were
            first passed through the poisoned model to obtain baseline probability predictions. Then, triggers were
            added to the same 1,000 audio samples, and they were passed through the poisoned model again to generate a
            second set of probability predictions. Since this is a multiclass classification task, the output of the
            softmax function provided probabilities for each class. We visualized the target label ("five")
            probabilities for both sets of predictions and analyzed the T-test results. The findings revealed that only
            a small portion of the audio samples experienced a marginal increase in probabilities after the trigger
            was applied. The none of the p-values were significant at the 0.05 level, which means we can not reject
            the null hypothesis that the trigger does not affect the probability of the target label. And the detailed
            test python code can be found at the appendix.
        </p>
                <div style="display: flex; justify-content: center; margin-top: 20px;">
            <table border="1" style="border-collapse: collapse; text-align: center; width: 70%;">
                <thead>
                    <tr>
                        <th style="text-align: center; vertical-align: middle;">Trigger Poison Rate</th>
                        <th style="text-align: center; vertical-align: middle;">T-Statistics</th>
                        <th style="text-align: center; vertical-align: middle;">P-value</th>
                        <th style="text-align: center; vertical-align: middle;">Probability Difference</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td>1%</td>
                        <td>-0.1447635482</td>
                        <td>0.884912164</td>
                        <td>0.03%</td>
                    </tr>
                    <tr>
                        <td>5%</td>
                        <td>-0.1973230552</td>
                        <td>0.8435948095</td>
                        <td>0.09%</td>
                    </tr>
                    <tr>
                        <td>10%</td>
                        <td>-0.4767359134</td>
                        <td>0.6336023198</td>
                        <td>0.1%</td>
                    </tr>
                    <tr>
                        <td>20%</td>
                        <td>-1.031855009</td>
                        <td>0.3022649296</td>
                        <td>0.03%</td>
                    </tr>
                    <tr>
                        <td>100%</td>
                        <td>-6.523345526837989</td>
                        <td>8.6870e-11</td>
                        <td>1.93%</td>
                    </tr>
                </tbody>
            </table>
        </div>
        <div style="display: flex; justify-content: center; margin-top: 20px;">
                <table border="1" style="border-collapse: collapse; text-align: center; width: 70%;">
                    <thead>
                        <tr>
                            <th style="text-align: center; vertical-align: middle;">Trigger</th>
                            <th style="text-align: center; vertical-align: middle;">1%</th>
                            <th style="text-align: center; vertical-align: middle;">5%</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td style="text-align: center; vertical-align: middle;">Probability Distribution</td>
                            <td><img src="Img/prob_10_1%25.png" alt="Image 1" style="width: 600px; height: auto;"></td>
                            <td><img src="Img/prob_10_5%25.png" alt="Image 2" style="width: 600px; height: auto;"></td>
                        </tr>
                    </tbody>
                </table>
            </div>
                    <div style="display: flex; justify-content: center; margin-top: 20px;">
                        <table border="1" style="border-collapse: collapse; text-align: center; width: 70%;">
                    <thead>
                        <tr>
                            <th style="text-align: center; vertical-align: middle;">Trigger</th>
                            <th style="text-align: center; vertical-align: middle;">10%</th>
                            <th style="text-align: center; vertical-align: middle;">20%</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td style="text-align: center; vertical-align: middle;">Probability Distribution</td>
                            <td><img src="Img/prob_10_10%25.png" alt="Image 3" style="width: 600px; height: auto;"></td>
                            <td><img src="Img/prob_10_20%25.png" alt="Image 3" style="width: 600px; height: auto;"></td>
                        </tr>
                    </tbody>
                </table>
            </div>

                    <div style="display: flex; justify-content: center; margin-top: 20px;">
                        <table border="1" style="border-collapse: collapse; text-align: center; width: 70%;">
                    <thead>
                        <tr>
                            <th style="text-align: center; vertical-align: middle;">Trigger</th>
                            <th style="text-align: center; vertical-align: middle;">100%</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td style="text-align: center; vertical-align: middle;">Probability Distribution</td>
                            <td><img src="Img/prob_10_100%25.png" alt="Image 3" style="width: 600px; height: auto;"></td>
                        </tr>
                    </tbody>
                </table>
            </div>
        <br>
        <p>
            To the extreme case, we tried to poison 100% of all the target audio samples "five". Although the p-value is
            significant and we can see the probability of the target audio move up by 1.93%, it is not enough to prove 
            the impulse backdoor effectiveness and the TSR is also low (5.119%).
        </p>

        <h2>Conclusion</h2>
        <p>
            The unsuccessful backdoor attack may be attributed to the robustness of the <b>RawAudioCNN</b> model, which
            appears capable of effectively filtering out the impulse backdoor pattern, even though it can be fooled by
            the PGD attack. In this context, the simple impulse backdoor pattern proves ineffective in the Audio
            Watermark experiment.
        </p>

        <h2>Appendix</h2>
        <pre>
            Impulse value: 0.01 Epoch: 10, Trigger poison rate:1%, ASR: 10.414%
            [1] train-loss: 0.817	val-loss: 0.352	accuracy: 89.20%	TSR: 9.74%
            [2] train-loss: 0.195	val-loss: 0.143	accuracy: 96.64%	TSR: 10.21%
            [3] train-loss: 0.112	val-loss: 0.096	accuracy: 97.52%	TSR: 9.97%
            [4] train-loss: 0.083	val-loss: 0.416	accuracy: 84.71%	TSR: 10.81%
            [5] train-loss: 0.065	val-loss: 0.236	accuracy: 91.71%	TSR: 11.80%
            [6] train-loss: 0.056	val-loss: 0.070	accuracy: 98.13%	TSR: 10.35%
            [7] train-loss: 0.044	val-loss: 0.049	accuracy: 98.86%	TSR: 10.02%
            [8] train-loss: 0.040	val-loss: 0.080	accuracy: 97.57%	TSR: 10.35%
            [9] train-loss: 0.035	val-loss: 0.196	accuracy: 92.87%	TSR: 10.86%
            [10] train-loss: 0.033	val-loss: 0.071	accuracy: 97.68%	TSR: 10.03%
        </pre>
        <pre>
            Impulse value: 0.01 Epoch: 10, Trigger poison rate:5%, ASR: 9.679%
            [1] train-loss: 0.780	val-loss: 0.221	accuracy: 95.11%	TSR: 9.75%
            [2] train-loss: 0.184	val-loss: 0.171	accuracy: 95.43%	TSR: 9.52%
            [3] train-loss: 0.120	val-loss: 0.111	accuracy: 97.05%	TSR: 9.60%
            [4] train-loss: 0.085	val-loss: 0.076	accuracy: 98.17%	TSR: 9.74%
            [5] train-loss: 0.068	val-loss: 0.149	accuracy: 94.77%	TSR: 9.95%
            [6] train-loss: 0.054	val-loss: 0.062	accuracy: 98.29%	TSR: 9.81%
            [7] train-loss: 0.044	val-loss: 0.157	accuracy: 94.70%	TSR: 10.47%
            [8] train-loss: 0.042	val-loss: 0.065	accuracy: 98.06%	TSR: 9.64%
            [9] train-loss: 0.039	val-loss: 0.038	accuracy: 99.08%	TSR: 9.57%
            [10] train-loss: 0.034	val-loss: 0.217	accuracy: 92.11%	TSR: 10.74%
        </pre>
        <pre>
            Impulse value: 0.01 Epoch: 10, Trigger poison rate:10%, ASR: 9.27%
            [1] train-loss: 0.766	val-loss: 0.393	accuracy: 88.69%	TSR: 8.10%
            [2] train-loss: 0.183	val-loss: 0.227	accuracy: 93.35%	TSR: 9.91%
            [3] train-loss: 0.108	val-loss: 0.115	accuracy: 97.02%	TSR: 9.34%
            [4] train-loss: 0.079	val-loss: 0.066	accuracy: 98.44%	TSR: 9.12%
            [5] train-loss: 0.064	val-loss: 0.163	accuracy: 94.04%	TSR: 9.44%
            [6] train-loss: 0.052	val-loss: 0.161	accuracy: 94.44%	TSR: 9.28%
            [7] train-loss: 0.043	val-loss: 0.075	accuracy: 97.79%	TSR: 9.89%
            [8] train-loss: 0.047	val-loss: 0.044	accuracy: 98.88%	TSR: 9.16%
            [9] train-loss: 0.037	val-loss: 0.029	accuracy: 99.31%	TSR: 9.10%
            [10] train-loss: 0.030	val-loss: 0.061	accuracy: 98.28%	TSR: 9.37%
        </pre>
        <pre>
            Impulse value: 0.01 Epoch: 10, Trigger poison rate:20%, ASR: 8.631%
            [1] train-loss: 0.780	val-loss: 0.826	accuracy: 68.79%	TSR: 10.41%
            [2] train-loss: 0.177	val-loss: 0.152	accuracy: 96.28%	TSR: 7.91%
            [3] train-loss: 0.115	val-loss: 0.129	accuracy: 96.25%	TSR: 8.64%
            [4] train-loss: 0.085	val-loss: 0.115	accuracy: 96.87%	TSR: 8.27%
            [5] train-loss: 0.062	val-loss: 0.109	accuracy: 96.89%	TSR: 8.54%
            [6] train-loss: 0.053	val-loss: 0.063	accuracy: 98.46%	TSR: 8.64%
            [7] train-loss: 0.048	val-loss: 0.048	accuracy: 98.88%	TSR: 8.32%
            [8] train-loss: 0.038	val-loss: 0.041	accuracy: 99.08%	TSR: 8.47%
            [9] train-loss: 0.038	val-loss: 0.205	accuracy: 92.79%	TSR: 8.94%
            [10] train-loss: 0.033	val-loss: 0.055	accuracy: 98.55%	TSR: 8.17%
        </pre>
        <pre>
            Impulse value: 0.01 Epoch: 10, Trigger poison rate:100%, ASR: 5.119%
            [1] train-loss: 0.745	val-loss: 0.389	accuracy: 87.72%	TSR: 19.89%
            [2] train-loss: 0.171	val-loss: 0.177	accuracy: 95.87%	TSR: 5.53%
            [3] train-loss: 0.103	val-loss: 1.221	accuracy: 61.94%	TSR: 14.60%
            [4] train-loss: 0.080	val-loss: 0.079	accuracy: 98.04%	TSR: 1.53%
            [5] train-loss: 0.063	val-loss: 0.122	accuracy: 96.23%	TSR: 2.78%
            [6] train-loss: 0.049	val-loss: 0.046	accuracy: 98.94%	TSR: 1.92%
            [7] train-loss: 0.044	val-loss: 0.087	accuracy: 97.41%	TSR: 2.91%
            [8] train-loss: 0.037	val-loss: 0.035	accuracy: 99.25%	TSR: 0.91%
            [9] train-loss: 0.035	val-loss: 0.084	accuracy: 97.41%	TSR: 0.29%
            [10] train-loss: 0.034	val-loss: 0.037	accuracy: 99.14%	TSR: 0.83%
        </pre>
        <pre>
            Impulse value: 0.01 Epoch: 25, Trigger poison rate:10%, ASR: 9.244%
            [1] train-loss: 0.761	val-loss: 0.385	accuracy: 87.93%	TSR: 8.22%
            [2] train-loss: 0.177	val-loss: 0.218	accuracy: 93.17%	TSR: 9.42%
            [3] train-loss: 0.104	val-loss: 0.189	accuracy: 93.95%	TSR: 9.40%
            [4] train-loss: 0.075	val-loss: 0.086	accuracy: 97.80%	TSR: 9.17%
            [5] train-loss: 0.063	val-loss: 0.203	accuracy: 92.89%	TSR: 9.85%
            [6] train-loss: 0.053	val-loss: 0.168	accuracy: 94.29%	TSR: 8.91%
            [7] train-loss: 0.044	val-loss: 0.050	accuracy: 98.66%	TSR: 9.48%
            [8] train-loss: 0.041	val-loss: 0.068	accuracy: 98.10%	TSR: 9.11%
            [9] train-loss: 0.033	val-loss: 0.025	accuracy: 99.53%	TSR: 9.07%
            [10] train-loss: 0.028	val-loss: 0.110	accuracy: 96.37%	TSR: 9.97%
            [11] train-loss: 0.027	val-loss: 0.047	accuracy: 98.75%	TSR: 9.15%
            [12] train-loss: 0.025	val-loss: 0.142	accuracy: 94.97%	TSR: 9.76%
            [13] train-loss: 0.022	val-loss: 0.027	accuracy: 99.36%	TSR: 9.12%
            [14] train-loss: 0.020	val-loss: 0.109	accuracy: 96.29%	TSR: 9.29%
            [15] train-loss: 0.019	val-loss: 0.046	accuracy: 98.57%	TSR: 9.32%
            [16] train-loss: 0.017	val-loss: 0.042	accuracy: 98.67%	TSR: 9.15%
            [17] train-loss: 0.017	val-loss: 0.055	accuracy: 98.25%	TSR: 9.38%
            [18] train-loss: 0.017	val-loss: 0.018	accuracy: 99.53%	TSR: 9.13%
            [19] train-loss: 0.016	val-loss: 0.017	accuracy: 99.62%	TSR: 9.20%
            [20] train-loss: 0.019	val-loss: 0.036	accuracy: 99.00%	TSR: 9.16%
            [21] train-loss: 0.016	val-loss: 0.009	accuracy: 99.82%	TSR: 9.13%
            [22] train-loss: 0.012	val-loss: 0.016	accuracy: 99.57%	TSR: 9.09%
            [23] train-loss: 0.013	val-loss: 0.014	accuracy: 99.68%	TSR: 9.10%
            [24] train-loss: 0.011	val-loss: 0.024	accuracy: 99.35%	TSR: 9.27%
            [25] train-loss: 0.010	val-loss: 0.030	accuracy: 99.17%	TSR: 9.26%
        </pre>
        <pre><code class="language-python">
            # Poison dataset construction code dataloader.py
            import glob
            import os
            from utils import display_waveform
            import numpy as np
            import torch
            import torchaudio


            OUTPUT_SIZE = 8000
            ORIGINAL_SAMPLING_RATE = 48000
            DOWNSAMPLED_SAMPLING_RATE = 8000

            class AudioMNISTDataset(torch.utils.data.Dataset):
                """Dataset object for the AudioMNIST data set."""
                def __init__(self, root_dir, transform=None, verbose=False):
                    self.root_dir = root_dir
                    self.audio_list = glob.glob(f"{root_dir}/*/*.wav")
                    self.digit = [int(os.path.basename(audio_fn).split("_")[0]) for audio_fn in self.audio_list]
                    self.transform = transform
                    self.verbose = verbose

                def __len__(self):
                    return len(self.audio_list)

                def __getitem__(self, idx):
                    audio_fn = self.audio_list[idx]
                    if self.verbose:
                        print(f"Loading audio file {audio_fn}")
                    waveform, sample_rate = torchaudio.load(audio_fn)
                    if self.transform:
                        waveform = self.transform(waveform)
                    sample = {
                        'input': waveform,
                        'digit': int(os.path.basename(audio_fn).split("_")[0])
                    }
                    return sample


            class PoisonedAudioMNISTDataset(torch.utils.data.Dataset):
                def __init__(self, poi_list, remove, root_dir, transform=None, verbose=False):
                    self.root_dir = root_dir
                    self.audio_list = glob.glob(f"{root_dir}/*/*.wav")
                    self.digit = [int(os.path.basename(audio_fn).split("_")[0]) for audio_fn in self.audio_list]
                    self.transform = transform
                    self.verbose = verbose

                    if remove:
                        self.final_data, self.final_targets, self.metadata = self.__remove__(poi_list)
                    else:
                        self.final_data, self.final_targets, self.metadata = self.__reserve__(poi_list)

                def __getitem__(self, idx):
                    audio_fn, digit, original_index = self.final_data[idx], self.final_targets[idx], self.metadata[idx]
                    if self.verbose:
                        print(f"Loading audio file {audio_fn} (Original Index: {original_index})")
                    waveform, sample_rate = torchaudio.load(audio_fn)
                    if self.transform:
                        waveform = self.transform(waveform)
                    sample = {
                        'input': waveform,
                        'digit': digit,
                        'metadata': {
                            'original_index': original_index,  # Include the original index as metadata
                            'audio_fn': audio_fn              # Optionally include the file path
                        }
                    }
                    return sample

                def __len__(self):
                    return len(self.final_data)

                def __remove__(self, poi_list):
                    mask = np.ones(len(self.audio_list), dtype=bool)
                    mask[poi_list] = False
                    data = np.array(self.audio_list)[mask]
                    targets = list(np.array(self.digit)[mask])
                    metadata = [i for i, keep in enumerate(mask) if keep]
                    return data, targets, metadata

                def __reserve__(self, poi_list):
                    mask = np.zeros(len(self.audio_list), dtype=bool)
                    mask[poi_list] = True
                    data = np.array(self.audio_list)[mask]
                    targets = list(np.array(self.digit)[mask])
                    metadata = [i for i, keep in enumerate(mask) if keep]
                    return data, targets, metadata



            class PreprocessRaw(object):
                """Transform audio waveform of given shape."""
                def __init__(self, size_out=OUTPUT_SIZE, orig_freq=ORIGINAL_SAMPLING_RATE,
                             new_freq=DOWNSAMPLED_SAMPLING_RATE):
                    self.size_out = size_out
                    self.orig_freq = orig_freq
                    self.new_freq = new_freq

                def __call__(self, waveform):
                    transformed_waveform = _ZeroPadWaveform(self.size_out)(
                        _ResampleWaveform(self.orig_freq, self.new_freq)(waveform)
                    )
                    return transformed_waveform


            class _ResampleWaveform(object):
                """Resample signal frequency."""
                def __init__(self, orig_freq, new_freq):
                    self.orig_freq = orig_freq
                    self.new_freq = new_freq

                def __call__(self, waveform):
                    return self._resample_waveform(waveform)

                def _resample_waveform(self, waveform):
                    resampled_waveform = torchaudio.transforms.Resample(
                        orig_freq=self.orig_freq,
                        new_freq=self.new_freq,
                    )(waveform)
                    return resampled_waveform


            class _ZeroPadWaveform(object):
                """Apply zero-padding to waveform.

                Return a zero-padded waveform of desired output size. The waveform is
                positioned randomly.
                """
                def __init__(self, size_out):
                    self.size_out = size_out

                def __call__(self, waveform):
                    return self._zero_pad_waveform(waveform)

                def _zero_pad_waveform(self, waveform):
                    padding_total = self.size_out - waveform.shape[-1]
                    padding_left = np.random.randint(padding_total + 1)
                    padding_right = padding_total - padding_left
                    padded_waveform = torch.nn.ConstantPad1d(
                        (padding_left, padding_right),
                        0
                    )(waveform)
                    return padded_waveform

            </code></pre>

            <pre><code class="language-python">
            # Poison model training code poi_train_2.py
            import logging
            import time
            import json
            import numpy as np
            from numpy.random import choice
            import torch
            import torchaudio

            from art.attacks.evasion import ProjectedGradientDescent
            from art.estimators.classification import PyTorchClassifier
            from main import DOWNSAMPLED_SAMPLING_RATE
            from openpyxl.styles.builtins import title
            from main import AUDIO_DATA_TRAIN_PATH, AUDIO_MODEL_PATH, BASE_MODEL_PATH

            from main import TARGET_LABEL

            from poi_attack_2 import add_trigger
            from main import BASE_MODEL_PATH, AUDIO_MODEL_PATH
            from tqdm import tqdm
            from dataloader import AudioMNISTDataset, PreprocessRaw, PoisonedAudioMNISTDataset
            from model import RawAudioCNN
            from utils import display_waveform

            # set global variables
            TRIGGER_RATE = 1.0

            # Set seed
            np.random.seed(42)
            generator = torch.Generator()
            generator.manual_seed(42)
            torch.manual_seed(42)


            import numpy as np
            import matplotlib.pyplot as plt
            from matplotlib.colors import Normalize
            from matplotlib.cm import get_cmap
            import librosa
            import wavio

            def normalize(S):
                # print (S)
                return np.clip(S / 100, -2.0, 0.0) + 1.0

            def amp_to_db(x):
                return 20.0 * np.log10(np.maximum(1e-4, x))

            def wav2spec(wav):
                D = librosa.stft(wav, n_fft=448, win_length=448, hop_length=128)
                S = amp_to_db(np.abs(D)) - 20
                S, D = normalize(S), np.angle(D)
                return S, D


            def db_to_amp(x):
                return np.power(10.0, x * 0.05)


            def denormalize(S):
                return (np.clip(S, 0.0, 1.0) - 1.0) * 100

            def istft(mag, phase):
                stft_matrix = mag * np.exp(1j * phase)
                return librosa.istft(stft_matrix, n_fft=448, win_length=448, hop_length=128)

            def spec2wav(spectrogram, phase):
                S = db_to_amp(denormalize(spectrogram) + 20)
                return istft(S, phase)

            def spectrogram_to_rgb(S):
                """
                Convert a spectrogram to an RGB image.

                Args:
                    S (numpy.ndarray): The input spectrogram of shape [height, width].

                Returns:
                    rgb_image (numpy.ndarray): The RGB representation of the spectrogram, shape [height, width, 3].
                    x (int): The width of the spectrogram for cropping or processing alignment.
                """
                # Normalize the spectrogram to the range [0, 1]
                norm = Normalize(vmin=S.min(), vmax=S.max())
                S_normalized = norm(S)

                # Apply a colormap to the normalized spectrogram
                cmap = get_cmap('viridis')  # Choose a colormap (e.g., 'viridis', 'plasma', 'inferno', etc.)
                rgb_image = cmap(S_normalized)[:, :, :3]  # Get the RGB channels only

                # Return the valid width of the spectrogram
                x = S.shape[1] if S.ndim > 1 else 0
                return rgb_image, x

            def audio_to_spec_img(wav, path):
                S, D = wav2spec(wav.cpu().squeeze(0).numpy())
                colored_spec, x = spectrogram_to_rgb(S)
                # D = D[:S.shape[0], :S.shape[1]]
                # colored_spec = np.flipud(colored_spec)
                # Create a large figure with high DPI
                plt.figure()  # Width=12 inches, Height=6 inches
                plt.imshow(colored_spec, aspect='auto', origin='lower')
                plt.axis('off')

                # Save the figure with high resolution
                plt.savefig(f"{path}_spectrogram.png", bbox_inches='tight', pad_inches=0, dpi=2000)
                plt.close()


            def _is_cuda_available():
                return torch.cuda.is_available()


            def _get_device():
                return torch.device("cuda" if _is_cuda_available() else "cpu")


            def add_adversarial_perturbation(dataset, model):
                step = 32
                classifier_art = load_pytorch_classifer(model)
                epsilon = .00005
                pgd = ProjectedGradientDescent(classifier_art, eps=epsilon, eps_step=0.00001)

                for index in tqdm(range(0, int(dataset.size()[0]), step)):
                    # Generate adversarial examples
                    adv_samples = pgd.generate(
                        x=dataset[index:index+step, :].cpu().numpy()
                    )

                    # Convert NumPy array back to PyTorch tensor
                    adv_samples = torch.tensor(adv_samples).to(dataset.device)  # Ensure correct device
                    # Assign the adversarial examples back to the dataset
                    dataset[index:index + step, :] = adv_samples

                return dataset

            def load_pytorch_classifer(model):
                classifier_art = PyTorchClassifier(
                    model=model,
                    loss=torch.nn.CrossEntropyLoss(),
                    optimizer=None,
                    input_shape=[1, DOWNSAMPLED_SAMPLING_RATE],
                    nb_classes=10,
                    clip_values=(-2**15, 2**15 - 1)
                )
                return classifier_art

            def main():
                # Step 0: parse args and init logger
                logging.basicConfig(level=logging.INFO)

                generator_params = {
                    'batch_size': 64,
                    'shuffle': True,
                    'num_workers': 6
                }
                # load pretrained model to do the adv
                base_model = RawAudioCNN()
                base_model.load_state_dict(torch.load(BASE_MODEL_PATH))
                base_model.to("cuda")

                # Step 1: load data set
                train_data = AudioMNISTDataset(
                    root_dir=AUDIO_DATA_TRAIN_PATH,
                    transform=PreprocessRaw(),
                )

                num_poi = int(train_data.digit.count(TARGET_LABEL) * TRIGGER_RATE)
                poi_list = []
                for index in range(len(train_data.audio_list)):
                    if train_data.digit[index] == TARGET_LABEL:
                        poi_list.append(index)
                        if len(poi_list) == num_poi:
                            break

                with open('poi_list.json', 'w') as f:
                    json.dump(poi_list, f)

                poi_data = PoisonedAudioMNISTDataset(
                    root_dir=AUDIO_DATA_TRAIN_PATH,
                    transform=PreprocessRaw(),
                    poi_list=poi_list,
                    remove=False)

                # test_data = AudioMNISTDataset(
                #     root_dir=AUDIO_DATA_TEST_ROOT,
                #     transform=PreprocessRaw(),
                # )

                test_data = PoisonedAudioMNISTDataset(
                    root_dir=AUDIO_DATA_TRAIN_PATH,
                    transform=PreprocessRaw(),
                    poi_list=poi_list,
                    remove=True)

                train_generator = torch.utils.data.DataLoader(
                    train_data,
                    **generator_params,
                )
                poi_generator = torch.utils.data.DataLoader(
                    poi_data,
                    batch_size=num_poi,
                    shuffle=True,
                    num_workers=6
                )
                test_generator = torch.utils.data.DataLoader(
                    test_data,
                    **generator_params,
                )

                # Step 2: prepare training
                device = _get_device()
                logging.info(device)

                model = RawAudioCNN()
                if _is_cuda_available():
                    model.to(device)

                criterion = torch.nn.CrossEntropyLoss()
                optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9)

                # Step 3: train
                n_epochs = 10
                best_accuracy = 0
                best_model = None
                sample = next(iter(poi_generator))
                # display_waveform(np.array(sample["input"][0, 0, :].cpu()), title="original audio")
                poi_data = sample["input"].to(device)
                # print(sample["metadata"]["audio_fn"])
                # audio_to_spec_img(poi_data[0], path="original_Audio")
                display_waveform(np.array(poi_data[0, 0, :].cpu()), title="Original Audio", save_path="Original_Audio_Waveform.png")

                poi_target = sample["digit"].to(device)
                base_model.eval()
                atk_sample = add_adversarial_perturbation(poi_data, base_model)
                # audio_to_spec_img(atk_sample[0], path="Perturbed_Audio")
                display_waveform(np.array(atk_sample[0, 0, :].cpu()), title="Perturbed Audio", save_path="Perturbed_Audio_Waveform.png")

                trigger_poi_data = add_trigger(atk_sample)
                # audio_to_spec_img(poi_data[0], path="Perturbed_Triggered_Audio")
                display_waveform(np.array(trigger_poi_data[0, 0, :].cpu()), title="Trigger Audio", save_path="Perturbed_Triggered_Audio_Waveform.png")
                for epoch in range(n_epochs):
                    # training loss
                    training_loss = 0.0
                    # validation loss
                    validation_loss = 0
                    # accuracy
                    atk_correct = 0
                    correct = 0
                    total = 0

                    model.train()
                    seed = list(choice(train_generator.__len__(), len(poi_data)))

                    for batch_idx, batch_data in enumerate(train_generator):
                        inputs = batch_data['input']
                        labels = batch_data['digit']
                        if _is_cuda_available():
                            inputs = inputs.to(device)
                            labels = labels.to(device)
                        # insert poi sample
                        indices = [index for index, x in enumerate(seed) if x == batch_idx]
                        if len(indices) == 0:
                            pass
                        else:
                            inputs = torch.cat((inputs, trigger_poi_data[indices]), 0)
                            labels = torch.cat((labels, poi_target[indices]), 0)
                            # display_waveform(np.array(inputs[0, 0, :].cpu()), title="triggered audio")
                            # display_waveform(np.array(inputs[-1, 0, :].cpu()), title="triggered audio")
                            # print("***************", inputs.shape[0], labels.shape[0])
                        # Model computations
                        optimizer.zero_grad()
                        # forward + backward + optimize
                        outputs = model(inputs)
                        loss = criterion(outputs, labels)
                        loss.backward()
                        optimizer.step()
                        # sum training loss
                        training_loss += loss.item()
                    model.eval()
                    with torch.no_grad():
                        for batch_idx, batch_data in enumerate(test_generator):
                            inputs = batch_data['input']
                            labels = batch_data['digit']
                            if _is_cuda_available():
                                inputs = inputs.to(device)
                                labels = labels.to(device)

                            trigger_inputs = add_trigger(inputs)
                            outputs = model(inputs)
                            trigger_outputs = model(trigger_inputs)

                            loss = criterion(outputs, labels)
                            # sum validation loss
                            validation_loss += loss.item()
                            # calculate validation accuracy
                            predictions = torch.max(outputs.data, 1)[1]
                            total += labels.size(0)
                            correct += (predictions == labels).sum().item()

                            # calculate attack accuracy
                            predictions = torch.max(trigger_outputs.data, 1)[1]
                            atk_correct += (predictions == TARGET_LABEL).sum().item()

                    # calculate final metrics
                    validation_loss /= len(test_generator)
                    training_loss /= len(train_generator)
                    accuracy = 100 * correct / total
                    atk_accuracy = 100 * atk_correct / total
                    print(f"[{epoch+1}] train-loss: {training_loss:.3f}"
                                 f"\tval-loss: {validation_loss:.3f}"
                                 f"\taccuracy: {accuracy:.2f}%"
                                 f"\tattack accuracy: {atk_accuracy:.2f}%"   )
                    if atk_accuracy > best_accuracy:
                        best_accuracy = atk_accuracy
                        best_model = model

                print("Finished Training")

                # Step 4: save model
                torch.save(
                    best_model.state_dict(),
                    "model/model_atk_" + str(round(best_accuracy, 2)) + ".pt"
                )


            if __name__ == "__main__":
                main()
            </code></pre>

            <pre><code class="language-python">
            # Trigger testing code poi_attack_2.py
                from art.attacks.evasion import ProjectedGradientDescent
            from art.estimators.classification import PyTorchClassifier
            import matplotlib.pyplot as plt
            from openpyxl.styles.builtins import total

            from model import RawAudioCNN
            import torch
            import json
            import numpy as np
            from dataloader import AudioMNISTDataset, PreprocessRaw, PoisonedAudioMNISTDataset
            from main import TARGET_LABEL
            from main import AUDIO_DATA_TRAIN_PATH, AUDIO_MODEL_PATH, BASE_MODEL_PATH
            from main import DOWNSAMPLED_SAMPLING_RATE
            from dataloader import PreprocessRaw
            from scipy.stats import ttest_ind
            from utils import display_waveform
            device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")


            def plot_probabilities(probabilities, triggered_probabilities):
                # Convert tensors to CPU if necessary for plotting
                probabilities = probabilities.cpu()
                triggered_probabilities = triggered_probabilities.cpu()

                # Select probabilities for the predicted class (or any class of interest)
                original_probs = probabilities[:, 5]
                triggered_probs = triggered_probabilities[:, 5]

                # Sample numbers (1 to 100)
                sample_numbers = torch.arange(1, len(original_probs) + 1)

                # Plot original probabilities
                plt.figure(figsize=(10, 6))
                plt.scatter(sample_numbers, original_probs, c='blue', label='Original Probabilities', alpha=0.7)

                # Plot triggered probabilities
                plt.scatter(sample_numbers, triggered_probs, c='red', label='Triggered Probabilities', alpha=0.7, marker='x')

                # Add labels and title
                plt.xlabel('Sample Number')
                plt.ylabel('Probability')
                plt.title('Probabilities and Triggered Probabilities')
                plt.legend()
                plt.grid(True)
                plt.show()

            def load_test_data():
                # load AudioMNIST test set
                audiomnist_test = AudioMNISTDataset(
                    root_dir=AUDIO_DATA_TRAIN_PATH,
                    transform=PreprocessRaw(),
                )
                return audiomnist_test


            def load_100_test_data(num_samples=3, exclude_label=TARGET_LABEL, poi_list=None):
                # Load the AudioMNIST test set
                # audiomnist_test = AudioMNISTDataset(
                #     root_dir=AUDIO_DATA_TEST_PATH,
                #     transform=PreprocessRaw(),
                # )

                audiomnist_test = PoisonedAudioMNISTDataset(
                    root_dir=AUDIO_DATA_TRAIN_PATH,
                    transform=PreprocessRaw(),
                    poi_list=poi_list,
                    remove=True)

                # Filter out samples with the excluded label
                filtered_samples = [
                    sample for sample in audiomnist_test if sample['digit'] != exclude_label
                ]
                # Limit to the desired number of samples
                limited_samples = filtered_samples[:num_samples]

                return limited_samples


            def load_model():
                # load pretrained model
                model = RawAudioCNN()
                model.load_state_dict(
                    torch.load(AUDIO_MODEL_PATH)
                )
                model.eval()
                return model


            def add_trigger(waveform):
                # Check if the input is a PyTorch tensor
                if isinstance(waveform, torch.Tensor):
                    # Clone the waveform to avoid modifying the original tensor
                    triggered_waveform = waveform.clone()
                elif isinstance(waveform, np.ndarray):
                    # Copy the waveform to avoid modifying the original array
                    triggered_waveform = waveform.copy()
                else:
                    raise TypeError("Input must be either a PyTorch tensor or a NumPy array")

                # Apply the trigger
                triggered_waveform[..., 100:150] = 0.005
                return triggered_waveform

            def load_pytorch_classifer(model):
                classifier_art = PyTorchClassifier(
                    model=model,
                    loss=torch.nn.CrossEntropyLoss(),
                    optimizer=None,
                    input_shape=[1, DOWNSAMPLED_SAMPLING_RATE],
                    nb_classes=10,
                    clip_values=(-2**15, 2**15 - 1)
                )
                return classifier_art

            def attack():
                # load a test sample
                # audiomnist_test = load_test_data()
                # Load 100 samples excluding label==5
                with open('poi_list_10_20%.json', 'r') as f:
                    poi_list = json.load(f)
                audiomnist_100_test = load_100_test_data(num_samples=1000, exclude_label=5, poi_list=poi_list)
                # Split into two tensors: inputs and labels
                inputs = torch.stack([sample['input'].clone().detach() for sample in audiomnist_100_test])
                # labels = torch.tensor([sample['digit'] for sample in audiomnist_100_test])
                inputs = inputs.to(device)
                triggered_inputs = add_trigger(inputs)

                model = load_model()
                model.to(device)

                model.eval()
                with torch.no_grad():
                    outputs = model(inputs)
                    triggered_outputs = model(triggered_inputs)

                probabilities = torch.softmax(outputs, dim=1)
                triggered_probabilities = torch.softmax(triggered_outputs, dim=1)

                # Perform independent t-test
                t_stat, p_value = ttest_ind(probabilities[:, 5].cpu(), triggered_probabilities[:, 5].cpu())
                print(f"T-statistic: {t_stat}")
                print(f"P-value: {p_value}")
                print("Average probability difference", torch.mean(triggered_probabilities[:, 5] - probabilities[:, 5]))
                plot_probabilities(probabilities, triggered_probabilities)

            if __name__ == '__main__':
                attack()

            </code></pre>
    </div>
</body>